<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis概述]]></title>
    <url>%2F2017%2F09%2F13%2FRedis%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[逃避未必能躲过，面对未必最难过在互联网时代的背景下，大数据带来的冲击，是的传统的关系型数据库结构以及数据类型无法应对，比例如大数据时代下的特点是3V(海量Volume，多样Variety，实时Velocity)，3高（高并发，高可用，高性能）。这样的特点也就是的许多菲关系型数据库NoSql（not only sql）的数据库应然而生。 NoSql的概述NoSql简述传统的关系型数据库都是基于关系来的一对一，一对多，多对多等，这样的模型对复杂的社交网络，推荐系统中，这些场景中更注重于一种关系图谱的构建，传统的关系型数据库做这个是非常复杂和困难的。所以，一些非关系型数据库出现来解决这些问题，常用的非关系型数据库有Redis，Memcache，Mongdb。 NoSql数据库模型简介聚合模型：KV键值对,BSON，列族，图形 在分布式数据库中CAP原理CAP+BASE传统的ACID分别是什么1234A (Atomicity) 原子性C (Consistency) 一致性I (Isolation) 独立性D (Durability) 持久性 分布式数据库CAP123C:Consistency（强一致性）A:Availability（可用性）P:Partition tolerance（分区容错性） 123CA: 传统Oracle数据库 AP: 大多数网站架构的选择 CP: Redis、Mongodb CAP的3进2CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类： 123CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。CP - 满足一致性，分区容忍必的系统，通常性能不是特别高。AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。 BASEBASE就是为了解决关系数据库强一致性引起的问题而引起的可用性降低而提出的解决方案。BASE其实是下面三个术语的缩写：123基本可用（Basically Available）软状态（Soft state）最终一致（Eventually consistent） 它的思想是通过让系统放松对某一时刻数据一致性的要求来换取系统整体伸缩性和性能上改观。为什么这么说呢，缘由就在于大型系统往往由于地域分布和极高性能的要求，不可能采用分布式事务来完成这些指标，要想获得这些指标，我们必须采用另外一种方式来完成，这里BASE就是解决这个问题的办法. NoSql数据库四大分类 KV键值：典型介绍 123新浪：BerkeleyDB+redis美团：redis+tair阿里、百度：memcache+redis 文档型数据库(bson格式比较多)：典型介绍 12CouchDBMongoDB 列存储数据库 123CassandraHBase分布式文件系统 图关系数据库它不是放图形的，放的是关系比如:朋友圈社交网络、广告推荐系统、社交网络，推荐系统等。专注于构建关系图谱 12Neo4JInfoGrid Redis入门介绍入门概述1.是什么Redis:REmote DIctionary Server(远程字典服务器)是完全开源免费的，用C语言编写的，遵守BSD协议，是一个高性能的(key/value)分布式内存数据库，基于内存运行,并支持持久化的NoSQL数据库，是当前最热门的NoSql数据库之一,也被人们称为数据结构服务器。Redis 与其他 key - value 缓存产品有以下三个特点: Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 2.能干嘛 内存存储和持久化：redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务 取最新N个数据的操作，如：可以将最新的10条评论的ID放在Redis的List集合里面 模拟类似于HttpSession这种需要设定过期时间的功能 发布、订阅消息系统 定时器、计数器 3.安装Linux版安装下载获得redis-3.0.4.tar.gz后将它放入我们的Linux目录/opt/opt目录下，解压命令:tar -zxvf redis-3.0.4.tar.gz解压完成后出现文件夹：redis-3.0.4进入目录:cd redis-3.0.4在redis-3.0.4目录下执行make命令 4.启动修改redis.conf文件将里面的daemonize no 改成 yes，让服务在后台启动将默认的redis.conf拷贝到自己定义好的一个路径下，比如/myconf启动: redis-server /myconf/redis.conf, redis-cli连通测试:/usr/local/bin目录下运行redis-server，运行拷贝出存放了自定义conf文件目录下的redis.conf文件 5.关闭12单实例关闭：redis-cli shutdown多实例关闭，指定端口关闭:redis-cli -p 6379 shutdown 6.Redis启动后杂项基础知识讲解 单进程:单进程模型来处理客户端的请求。对读写等事件的响应,是通过对epoll函数的包装来做到的。Redis的实际处理速度完全依靠主进程的执行效率,epoll是Linux内核为处理大批量文件描述符而作了改进的epoll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。 默认16个数据库，类似数组下表从零开始，初始默认使用零号库 select命令切换数据库 dbsize查看当前数据库的key的数量 flushdb：清空当前库 Flushall:通杀全部库 统一密码管理，16个库都是同样密码，要么都OK要么一个也连接不上 Redis索引都是从零开始 为什么默认端口是6379 Redis数据类型一、Redis的五大数据类型1.string（字符串） string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。 string类型是Redis最基本的数据类型，一个redis中字符串value最多可以是512M。 2.hash（哈希，类似java里的Map） Redis hash 是一个键值对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似Java里面的Map 3.list（列表） Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。 底层实际是个链表。 4.set（集合） Redis的Set是string类型的无序集合。它是通过HashTable实现实现的。 5.zset(sorted set：有序集合) Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。 redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。 二、常用命令（官网常用命令)键（keys）123456789101112#获取当前库的全部键keys *#判断某个key是否存在exists key#当前库就没有了，被移除了move key db#为给定的key设置过期时间expire key 秒钟#查看还有多少秒过期，-1表示永不过期，-2表示已过期ttl key #查看你的key是什么类型type key 字符串（String）它是单值单value的12345678910111213141516set/get/del/append/strlen#一定要是数字才能进行加减Incr/decr/incrby/decrby#获取指定区间范围内的值，类似between......and的关系，从零到负一表示全部getrange#设置指定区间范围内的值，格式是setrange key值 具体值setrangesetex(set with expire)键秒值/setnx(set if not exist)#同时设置一个或多个 key-value 对mset#获取所有(一个或多个)给定 key 的值mget#同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在msetnx#先get再setgetset Redis列表(List)它是单值多value1234567891011lpush/rpush/lrangelpop/rpoplindex，按照索引下标获得元素(从上到下)llen#删N个valuelrem key #截取指定范围的值后再赋值给keyltrim key 开始index 结束indexrpoplpush 源列表 目的列表lset key index valuelinsert key before/after 值1 值2 性能总结:它是一个字符串链表，left、right都可以插入添加；如果键不存在，创建新的链表；如果键已存在，新增内容；如果值全移除，对应的键也就消失了。链表的操作无论是头和尾效率都极高，但假如是对中间元素进行操作，效率就很惨淡了。 Redis集合(Set)它是单值多value 1234567891011121314151617sadd/smembers/sismember#获取集合里面的元素个数scard#删除集合中元素srem key value #如果超过最大数量就全部取出，如果写的值是负数，比如-3 ，表示需要取出3个，但是可能会有重复值。srandmember key 某个整数(随机出几个数)#随机出栈spop key#作用是将key1里的某个值赋给key2smove key1 key2 在key1里某个值#在第一个set里面而不在后面任何一个set里面的项差集：sdiff#在第一个set里面并且在后面任何一个set里面的项交集：sinter#在第一个set里面或者在后面任何一个set里面的项并集：sunion Redis哈希(Hash)KV模式不变，但V是一个键值对 123456hset/hget/hmset/hmget/hgetall/hdelhlenhexists key 在key里面的某个值的keyhkeys/hvalshincrby/hincrbyfloathsetnx Redis有序集合Zset(sorted set)在set基础上，加一个score值。之前set是k1 v1 v2 v3，现在zset是k1 score1 v1 score2 v2 123456789101112131415zadd/zrange zrem key 某score下对应的value值，作用是删除元素#获取集合中元素个数zcard#获取分数区间内元素个数zcountzcount key 开始分数区间 结束分数区间#获取value在zset中的下标位置zrank#按照值获得对应的分数zscore#作用是逆序获得下标值zrevrank key values值，zrevrangezrevrangebyscore key 结束score 开始score 解析配置文件redis.conf GENERAL通用 12345678910111213141516171819daemonize:默认为no，改为yespidfileporttcp-backlog:设置tcp的backlog，backlog其实是一个连接队列，backlog队列总和=未完成三次握手队列 + 已经完成三次握手队列。在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。注意Linux内核会将这个值减小到/proc/sys/net/core/somaxconn的值，所以需要确认增大somaxconn和tcp_max_syn_backlog两个值来达到想要的效果timeoutbind #单位为秒，如果设置为0，则不会进行Keepalive检测，建议设置成60 tcp-keepalive#日志级别loglevel#日志文件名logfile#是否把日志输出到syslog中syslog-enabled#指定syslog里的日志标志syslog-ident#指定syslog设备，值可以是USER或LOCAL0-LOCAL7syslog-facilitydatabases SNAPSHOTTING快照 12345678save save 秒钟 写操作次数 禁用stop-writes-on-bgsave-errorrdbcompressionrdbchecksumdbfilenamedir REPLICATION复制 SECURITY安全访问密码的查看、设置和取消 LIMITS限制 123456789101112131415161718192021#最大客户端个数maxclients#允许最大使用内存maxmemory#内存过期清除策略maxmemory-policy #使用LRU算法移除key，只对设置了过期时间的键 (1)volatile-lru #使用LRU算法移除key (2)allkeys-lru #在过期集合中移除随机的key，只对设置了过期时间的键 (3)volatile-random #移除随机的key (4)allkeys-random #移除那些TTL值最小的key，即那些最近要过期的key (5)volatile-ttl #不进行移除。针对写操作，只是返回错误信息 (6)noeviction#设置样本数量，LRU算法和最小TTL算法都并非是精确的算法，而是估算值，所以#你可以设置样本的大小，redis默认会检查这么多个key并选择其中LRU的那个。maxmemory-samples APPEND ONLY MODE追加 123456789101112131415161718#是否打开appendonly#文件名称appendfilename#追加策略appendfsync #同步持久化 每次发生数据变更会被立即记录到磁盘 性能较差但数据完整性比较好 (1)always #出厂默认推荐，异步操作，每秒记录 如果一秒内宕机，有数据丢失 (2)everysec #不追加，不推荐 (3)no#重写时是否可以运用Appendfsync，用默认no即可，保证数据安全性。no-appendfsync-on-rewrite #设置重写的基准值 (1)auto-aof-rewrite-min-size #设置重写的基准值 (2)auto-aof-rewrite-percentage 常见配置redis.conf介绍总结 Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程 1daemonize no 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定。 1pidfile /var/run/redis.pid 指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字。 1port 6379 绑定的主机地址。 1bind 127.0.0.1 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能。 1timeout 300 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose。 1loglevel verbose 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null。 1logfile stdout 设置数据库的数量，默认数据库为0，可以使用SELECT &lt;dbid&gt;命令在连接上指定数据库id。 1databases 16 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合。 1save &lt;seconds&gt; &lt;changes&gt; Redis默认配置文件中提供了三个条件： 123456#900秒（15分钟）内有1个更改save 900 1#300秒（5分钟)内有10个更改save 300 10#60秒内有10000个更改save 60 10000 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大。 1rdbcompression yes 指定本地数据库文件名，默认值为dump.rdb。 1dbfilename dump.rdb 指定本地数据库存放目录。 1dir ./ 设置当本机为slave服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步。 1slaveof &lt;masterip&gt; &lt;masterport&gt; 当master服务设置了密码保护时，slave服务连接master的密码。 1masterauth &lt;master-password&gt; 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭。 1requirepass foobared 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息。 1maxclients 128 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区。 1maxmemory &lt;bytes&gt; 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no。 1appendonly no 指定更新日志文件名，默认为appendonly.aof 1appendfilename appendonly.aof 指定更新日志条件，共有3个可选值： 1234appendfsync everysecno：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折衷，默认值） 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制）。 1vm-enabled no 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享。 1vm-swap-file /tmp/redis.swap 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0。 1vm-max-memory 0 Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值。 1vm-page-size 32 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。 1vm-pages 134217728 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4。 1vm-max-threads 4 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启。 1glueoutputbuf yes 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法。 12hash-max-zipmap-entries 64hash-max-zipmap-value 512 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍）。 1activerehashing yes 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件。 1include /path/to/local.conf redis的持久化RDB（Redis DataBase）在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。它可以手动执行，也可以再redis.conf中配置，定期执行。Redis会单独创（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等）数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程。rdb保存的默认是dump.rdb文件，它是经过压缩的二进制文件。 RDB的缺点是最后一次持久化后的数据可能丢失。 自动保存间隔BGSAVE可以在不阻塞主进程的情况下完成数据的备份。可以通过redis.conf中设置多个自动保存条件，只要有一个条件被满足，服务器就会执行BGSAVE命令。 123456789# 以下配置表示的条件：# 服务器在900秒之内被修改了1次save 900 1# 服务器在300秒之内被修改了10次save 300 10# 服务器在60秒之内被修改了10000次save 60 10000#表示不同步save “” 如何触发（创建）RDB快照冷拷贝后重新使用 可以cp dump.rdb dump_new.rdbRDB文件可以通过两个命令来生成：命令save或者是bgsave Save：save时只管保存，其它不管，全部阻塞。 BGSAVE：Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求。即派生(fork)一个子进程来创建新的RDB文件，记录接收到BGSAVE当时的数据库状态，父进程继续处理接收到的命令，子进程完成文件的创建之后，会发送信号给父进程，而与此同时，父进程处理命令的同时，通过轮询来接收子进程的信号。可以通过lastsave命令获取最后一次成功执行快照的时间，执行flushall命令，也会产生dump.rdb文件，但里面是空的，无意义。 如何恢复而RDB文件的载入一般情况是自动的，redis服务器启动的时候，redis服务器再启动的时候如果检测到RDB文件的存在，那么redis会自动载入这个文件。 如何停止动态所有停止RDB保存规则的方法：redis-cli config set save &quot;&quot; 优劣势 适合大规模的数据恢复 对数据完整性和一致性要求不高 在一定间隔时间做一次备份，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑 AOF（Append Only File）以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作Aof保存的是appendonly.aof文件。 AOF启动/修复/恢复正常恢复 启动：设置Yes，修改默认的appendonly no，改为yes 将有数据的aof文件复制一份保存到对应目录(config get dir) 恢复：重启redis然后重新加载异常恢复 启动：设置Yes，修改默认的appendonly no，改为yes 备份被写坏的AOF文件 修复：redis-check-aof --fix进行修复 恢复：重启redis然后重新加载 rewriteAOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制,当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集，可以使用命令bgrewriteaof。重写原理AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，遍历新进程的内存中数据，每条记录有一条的Set语句。重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似。 触发机制Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发优势 每修改同步：appendfsync always 同步持久化 每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性比较好 每秒同步：appendfsync everysec异步操作，每秒记录 如果一秒内宕机，有数据丢失 不同步：appendfsync no 从不同步劣势 相同数据集的数据而言aof文件要远大于rdb文件，恢复速度慢于rdb。 aof运行效率要慢于rdb,每秒同步策略效率较好，不同步效率和rdb相同。 官网建议RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大只做缓存：如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式.同时开启两种持久化方式在这种情况下,当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整.RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。那要不要只使用AOF呢？作者建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)，快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。 性能建议因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这条规则。如果Enalbe AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了。代价一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上。默认超过原大小100%大小时重写可以改到适当的数值。如果不Enable AOF ，仅靠Master-Slave Replication 实现高可用性也可以。能省掉一大笔IO也减少了rewrite时带来的系统波动。代价是如果Master/Slave同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个Master/Slave中的RDB文件，载入较新的那个。新浪微博就选用了这种架构。 Redis的事务可以一次执行多个命令，本质是一组命令的集合。一个事务中的所有命令都会序列化，按顺序地串行化执行而不会被其它命令插入，不许加塞，一个队列中，一次性、顺序性、排他性的执行一系列命令。 常见案例(1)正常执行(2)放弃事务–&gt;中途放弃事物(3)全体连坐–&gt;如果中间有“错误”则全部放弃执行(4)冤头债主–&gt;执行过程中遇到某个不可执行的操作，则其他可以执行的放行。(5)watch监控 悲观锁/乐观锁/CAS(Check And Set)悲观锁：提交版本必须大于记录当前版本才能执行更新，传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。案例: 初始化信用卡可用余额和欠额，无加塞篡改，先监控再开启multi，保证两笔金额变动在同一个事务内，有加塞篡改，监控了key，如果key被修改了，后面一个事务的执行失效，unwatch一旦执行了exec之前加的监控锁都会被取消掉了。CSA 小结Watch指令，类似乐观锁，事务提交时，如果Key的值已被别的客户端改变，比如某个list已被别的客户端push/pop过了，整个事务队列都不会被执行通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化，EXEC命令执行的事务都将被放弃，同时返回Nullmulti-bulk应答以通知调用者事务执行失败。 3阶段 开启：以MULTI开始一个事务 入队：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面 执行：由EXEC命令触发事务 3特性 单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题。 不保证原子性：redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚。 Redis的发布订阅进程间的一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。先订阅后发布后才能收到消息12345678#可以一次性订阅多个SUBSCRIBE c1 c2 c3#消息发布PUBLISH c2 hello-redis#订阅多个，通配符*PSUBSCRIBE new *#收取消息PUBLISH new1 redis2015 Redis的复制(Master/Slave)也就是我们所说的主从复制，主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主,可实现读写分离，容灾恢复。 实现主从复制需修改配置文件细节操作：拷贝多个redis.conf文件，开启daemonize yes,pid文件名字,指定端口,log文件名字,dump.rdb名字原则：配从(库)不配主(库)从库配置1slaveof 主库IP 主库端口 每次与master断开之后，都需要重新连接，除非你配置进`redis.conf`文件，可以使用`info replication`可以查看当前redis的主从信息。 复制原理PSYNC命令来执行复制时的同步操作，包括完整重同步和部分重同步两种模式：完整重同步：通过让主服务器创建并发送RDB文件，以及向从服务器发送保存在缓存区里面的写命令在进行同步。部分重同步：用于处理断线后重复制的情况。当从服务器在断线后重新连接主服务器时，如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只要接收并执行这些写命令，就可以将数据库更新至主服务器当前所处的状态。部分重同步涉及到以下三点：123复制偏移量复制积压缓冲区服务器运行ID 复制偏移量执行复制的主从双方会分别维护一个复制偏移量，主服务器每次向从服务器发送N个字节数据时，就将自己的复制偏移量的值加上N，从服务器每接收到主服务器发送过来的N个字节数据时，也会将自己的偏移量加上N。这样，如果主从服务器处于一致的状态时，那么主从服务器的复制偏移量总是相同的。 复制积压缓冲区如果由于断线后，主从服务器重新进行连接时，发现复制偏移量不一致了，这时就需要用到复制积压缓冲区了。复制积压缓冲区是由主服务器维护的一个固定长度、先进先出的队列，默认大小是1MB。当主服务器进行命令传播时，它不仅会将写命令发送给所有从服务器，还会将写命令写入复制积压缓冲区里面。这样主服务器的复制积压缓冲区就会保存着最近传播的写命令，并且记录着相应的复制偏移量。当从服务器重新连接上主服务器时，如果发现复制偏移量不一致，就会在复制积压缓冲区中寻找对应偏移量之后的数据。如果该偏移量也不存在复制积压缓冲区中，那么只能进行完整重同步操作了。由以上描述可知，复制积压缓冲区的大小在网络不稳定的环境中，会大大影响集群的性能。所以合理设置复制积压缓冲区是调优的一种手段，对应的具体参数是：repl-backlog-size 服务器运行ID每一个Redis服务器，无论主从，都有一个由40个随机的十六进制的字符组成的运行ID。该ID在复制的时候，用于识别主服务器是否已经更改：如果主服务器的ID跟之前复制的不一致，则说明主服务器已经发生变更。这时需要进行完整重同步。但是只要是重新连接master,一次完全同步（全量复制)将被自动执行。 哨兵模式(sentinel)其实就是能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库例如：79带着80、81自定义的/myredis目录下新建sentinel.conf文件配置哨兵,填写内容 1sentinel monitor 被监控数据库名字(自己起名字) 127.0.0.1 6379 1 上面最后一个数字1，表示主机挂掉后salve投票看让谁接替成为主机，得票数多少后成为主机启动哨兵1redis-sentinel /myredis/sentinel.conf 一组sentinel能同时监控多个Master 复制的缺点复制延时：由于所有的写操作都是先在Master上操作，然后同步更新到Slave上，所以从Master同步到Slave机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重。 Redis的Java客户端JedisJedis所需要的jar包12commons-pool-1.6.jarjedis-2.1.0.jar Jedis常用操作 测试连通性 一个key 五大数据类型 事务提交 123456789101112131415161718192021222324252627282930package com.atguigu.redis.test;import redis.clients.jedis.Jedis;import redis.clients.jedis.Response;import redis.clients.jedis.Transaction;public class Test03 &#123; public static void main(String[] args) &#123; Jedis jedis = new Jedis("127.0.0.1",6379); //监控key，如果该动了事务就被放弃 jedis.watch("serialNum"); jedis.set("serialNum","s#####################"); jedis.unwatch();*/ //被当作一个命令进行执行 Transaction transaction = jedis.multi(); Response&lt;String&gt; response = transaction.get("serialNum"); transaction.set("serialNum","s002"); response = transaction.get("serialNum"); transaction.lpush("list3","a"); transaction.lpush("list3","b"); transaction.lpush("list3","c"); transaction.exec(); //2 transaction.discard(); System.out.println("serialNum***********"+response.get()); &#125;&#125; 加锁 1234567891011121314151617181920212223242526272829303132333435363738public class TestTransaction &#123; public boolean transMethod() &#123; Jedis jedis = new Jedis("127.0.0.1", 6379); int balance;// 可用余额 int debt;// 欠额 int amtToSubtract = 10;// 实刷额度 jedis.watch("balance"); //jedis.set("balance","5");//此句不该出现，讲课方便。模拟其他程序已经修改了该条目 balance = Integer.parseInt(jedis.get("balance")); if (balance &lt; amtToSubtract) &#123; jedis.unwatch(); System.out.println("modify"); return false; &#125; else &#123; System.out.println("***********transaction"); Transaction transaction = jedis.multi(); transaction.decrBy("balance", amtToSubtract); transaction.incrBy("debt", amtToSubtract); transaction.exec(); balance = Integer.parseInt(jedis.get("balance")); debt = Integer.parseInt(jedis.get("debt")); System.out.println("*******" + balance); System.out.println("*******" + debt); return true; &#125; &#125; /** * 通俗点讲，watch命令就是标记一个键，如果标记了一个键， 在提交事务前如果该键被别人修改过，那事务就会失败，这种情况通常可以在程序中 * 重新再尝试一次。 * 首先标记了键balance，然后检查余额是否足够，不足就取消标记，并不做扣减； 足够的话，就启动事务进行更新操作， * 如果在此期间键balance被其它人修改， 那在提交事务（执行exec）时就会报错， 程序中通常可以捕获这类错误再重新执行一次，直到成功。 */ public static void main(String[] args) &#123; TestTransaction test = new TestTransaction(); boolean retValue = test.transMethod(); System.out.println("main retValue-------: " + retValue); &#125;&#125; 主从复制6379,6380启动，先各自先独立，主写，从读 123456789public static void main(String[] args) throws InterruptedException &#123; Jedis jedis_M = new Jedis("127.0.0.1",6379); Jedis jedis_S = new Jedis("127.0.0.1",6380); jedis_S.slaveof("127.0.0.1",6379); jedis_M.set("k6","v6"); Thread.sleep(500); System.out.println(jedis_S.get("k6"));&#125; JedisPool线程池获取Jedis实例需要从JedisPool中获取，用完Jedis实例需要返还给JedisPool，如果Jedis在使用过程中出错，则也需要还给JedisPool，案例见代码: JedisPoolUtil 123456789101112131415161718192021222324252627282930313233343536373839package com.atguigu.redis.test;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class JedisPoolUtil &#123;//被volatile修饰的变量不会被本地线程缓存，对该变量的读写都是直接操作共享内存。private static volatile JedisPool jedisPool = null; private JedisPoolUtil() &#123;&#125; public static JedisPool getJedisPoolInstance()&#123; if(null == jedisPool)&#123; synchronized (JedisPoolUtil.class)&#123; if(null == jedisPool)&#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxActive(1000); poolConfig.setMaxIdle(32); poolConfig.setMaxWait(100*1000); poolConfig.setTestOnBorrow(true); jedisPool = new JedisPool(poolConfig,"127.0.0.1"); &#125; &#125; &#125; return jedisPool; &#125; public static void release(JedisPool jedisPool,Jedis jedis)&#123; if(null != jedis)&#123; jedisPool.returnResourceObject(jedis); &#125; &#125;&#125; 获取一个连接jedisPool.getResource(); 1234567891011121314151617181920package com.atguigu.redis.test; import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;public class Test01 &#123; public static void main(String[] args) &#123; JedisPool jedisPool = JedisPoolUtil.getJedisPoolInstance(); Jedis jedis = null; try &#123; jedis = jedisPool.getResource(); jedis.set("k18","v183"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; JedisPoolUtil.release(jedisPool, jedis); &#125; &#125;&#125; JedisPoolConfig配置总结JedisPool的配置参数大部分是由JedisPoolConfig的对应项来赋值的。 1234567891011121314151617181920212223242526272829303132333435#控制一个pool可分配多少个jedis实例，通过pool.getResource()来获取；如果赋值为-1，则表示不限制；如果pool已经分配了maxActive个jedis实例，则此时pool的状态为exhausted。maxActive#控制一个pool最多有多少个状态为idle(空闲)的jedis实例maxIdle：#表示当pool中的jedis实例都被allocated完时，pool要采取的操作；默认有三种。whenExhaustedAction：#表示无jedis实例时，直接抛出NoSuchElementException；WHEN_EXHAUSTED_FAIL#则表示阻塞住，或者达到maxWait时抛出JedisConnectionException；WHEN_EXHAUSTED_BLOCK#则表示新建一个jedis实例，也就说设置的maxActive无用； WHEN_EXHAUSTED_GROW#表示当borrow一个jedis实例时，最大的等待时间，如果超过等待时间，则直接抛JedisConnectionException；maxWait#获得一个jedis实例的时候是否检查连接可用性（ping()）；如果为true，则得到的jedis实例均是可用的；testOnBorrow#return 一个jedis实例给pool时，是否检查连接可用性（ping()）；testOnReturn#如果为true，表示有一个idle object evitor线程对idle object进行扫描，如果validate失败，此object会被从pool中drop掉；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义；testWhileIdle#表示idle object evitor两次扫描之间要sleep的毫秒数；timeBetweenEvictionRunsMillis#表示idle object evitor每次扫描的最多的对象数；numTestsPerEvictionRun#表示一个对象至少停留在idle状态的最短时间，然后才能被idle object evitor扫描并驱逐；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义；minEvictableIdleTimeMillis#在minEvictableIdleTimeMillis基础上，加入了至少minIdle个对象已经在pool里面了。如果为-1，evicted不会根据idle time驱逐任何对象。如果minEvictableIdleTimeMillis&gt;0，则此项设置无意义，且只有在timeBetweenEvictionRunsMillis大于0时才有意义；softMinEvictableIdleTimeMillis#borrowObject返回对象时，是采用DEFAULT_LIFO（last in first out，即类似cache的最频繁使用队列），如果为False，则表示FIFO队列；lifo#其中JedisPoolConfig对一些参数的默认设置如下：testWhileIdle=trueminEvictableIdleTimeMills=60000timeBetweenEvictionRunsMillis=30000numTestsPerEvictionRun=-1]]></content>
      <categories>
        <category>数据库</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>分布式缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql主从数据库的配置]]></title>
    <url>%2F2017%2F09%2F02%2FTomcat%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[没有实力的发怒，是毫无意义的因为实验室的项目需要。我们在自己写项目的时候数据库往往是单个的，是没有容灾备份，没有单点故障的处理，今天我们就说一下我们常用的数据库Mysql的主从备份。]]></content>
      <categories>
        <category>数据库</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>主从复制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[致谢606实验室]]></title>
    <url>%2F2017%2F06%2F07%2F%E8%87%B4%E8%B0%A2606%E5%AE%9E%E9%AA%8C%E5%AE%A4%2F</url>
    <content type="text"><![CDATA[国事如今谁倚仗，衣带一江而已。待续]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>EGOV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我和Hexo的秘密]]></title>
    <url>%2F2017%2F06%2F06%2F%E6%88%91%E5%92%8Chexo%E7%9A%84%E7%A7%98%E5%AF%86%2F</url>
    <content type="text"><![CDATA[劝君莫惜金缕衣，劝君惜取少年时2017年5月29，今天我终于有了自己的博客，开始把其他上面的东西往hexo上搬了，知乎啊，微博啊，CSDN，segmentflaut等上面的东西一点一点的搬过来。 流量统计下面的人数统计和访问统计是用的busuanzi 阅读次数文章的阅读次数用的是百度统计和leanCloud 图床本站所有的图片都是存储在免费的CDN服务中。Cloudinary提供的图片CDN服务，在Cloudinary中上传图片后，会生成对应的url地址，将地址直接拿来引用即可。 站内搜索本站的站内搜索是采用hexo自带的local search 引用站内文章可以通过内置标签post_link实现 1&#123;% post_link 文章文件名（不要后缀） 文章标题（可选） %&#125; 例如 引用hello.md 1&#123;% post_link Hello %&#125; 或者 1&#123;% post_link Hello 你好 %&#125; 常用命令本地部署，在localhost：4000 1hexo s 清除无用的标签，索引，分类 1hexo clean 更新部署 1hexo d -g]]></content>
      <categories>
        <category>Hexo相关</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC的执行流程]]></title>
    <url>%2F2017%2F06%2F05%2FSpringMVC%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[三更灯火五更鸡，正是男儿读书时在整个Spring MVC 框架中，DispatcherServlet 处于核心位置，负责协调和组织不同组件以完成请求处理并返回响应的工作。 Spring MVC 工作流程图 SpringMVC 处理请求过程： 用户向服务器发送请求，请求被Spring 前端控制Servelt DispatcherServlet捕获； DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回。(DispatcherServlet（中央调度），负责request和response，负责调用处理器映射器查找Handler，负责调用处理器适配器执行Handler，有了前端控制器降低了各个组件之间的耦合性，系统扩展性提高)。 DispatcherServlet 根据获得的Handler，选择一个合适的HandlerAdapter。（附注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler(…)方法） 提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作： HttpMessageConveter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息 数据转换：对请求消息进行数据转换。如String转换成Integer、Double等 数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等 数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中 Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象； 根据返回的ModelAndView，选择一个适合的ViewResolver（必须是已经注册到Spring容器中的ViewResolver，jsp还是pdf)返回给DispatcherServlet ； ViewResolver 结合Model和View，来渲染视图 将渲染结果返回给客户端。 Spring MVC工作流程图 UML时序图 组件说明:以下组件通常使用框架提供实现： DispatcherServlet：作为前端控制器，整个流程控制的中心，控制其它组件执行，统一调度，降低组件之间的耦合性，提高每个组件的扩展性。 HandlerMapping：通过扩展处理器映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 HandlAdapter：通过扩展处理器适配器，支持更多类型的处理器。 ViewResolver：通过扩展视图解析器，支持更多类型的视图解析，例如：jsp、freemarker、pdf、excel等。]]></content>
      <categories>
        <category>Web开发框架</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>SpringMVC</tag>
        <tag>SpringMVC执行流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark On YARN集群环境搭建]]></title>
    <url>%2F2017%2F06%2F02%2FSpark-On-YARN%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[尽人事，听天命 最近因为写论文的实验需要用到Spark集群，所以就需要自己动手配置一下，尽管现在有很多的云平台提供了很好的云服务，可以很方面的使用，但是收费还是很高的。自己亲自配置一下，才知道其实并不是很难，废话不多说，下面进入正题。 写在前面用户尽量使用带有root权限的用户，这里假设每个机器的用户是spark，可以减少不必要的麻烦。如何创建root权限用户，这里就不说了，每个节点上的用户都是一样的，安装的路径也必须是一致的。 软件准备： Jdk Scala Hadoop Spark 这里的版本，大家可以选择最新的版本就行。jdk 和 scala 的安装和配置大同小异， 修改主机名我们的目标是用主机名来代替主机IP，假设我们现在的机器有三台，我们选择其中一台作为master节点，其他两台作为worker节点，他们的IP为： 10.1.130.2110.1.130.2210.1.130.23 这里，我们想把21作为主节点，22，23作为工作节点。首先是要把每个机器的主机名（hostname）改一下，方法是 vi /etc/hostname 21节点： 110.1.130.21 master 22节点： 110.1.130.22 salve1 23节点： 110.1.130.23 slave2 配置完主机名之后，需要重启生效。 然后就是修改每个节点的hosts文件，方法是vi /etc/hosts 每个节点都做同样的修改，添加如下配置： 12310.1.130.21 master10.1.130.22 salve110.1.130.23 slave2 配置完成后，需要互ping一下，检查是否成功， 1spark@master$ ping slave1 #从master ping slave1，其他的类似 SSH免密登录如果机器没有安装ssh服务可以安装一下： 1sudo apt-get install openssh-server 免密登录，可以参考之间的方法 SSH免密登录 Java和Scala的安装以下所有的安装都是在master节点上进行的。 从官网下载最新版Java和Scala就可以，它们的安装方式差不多， Java的安装在你想要的安装的目录下解压,这里我们在自己用户下新建一个文件夹叫做app，注意不要用sudo来建立,直接在每一个节点下mkdir app就行了，将源文件放在app中，然后解压。 12cd apptar -zxvf jdk-7u75-linux-x64.gz 修改环境变量vi .bashrc, 添加下列内容,注意这里spark是你的用户名，即在你的户用文件夹下。 12345export WORK_SPACE=/home/spark/app/export JAVA_HOME=$WORK_SPACE/jdk1.7.0_75export JRE_HOME=/home/spark/work/jdk1.7.0_75/jreexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=$CLASSPATH:.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib 生效环境变量 1$ source .bashrc 查看java版本，如果打印出如下版本信息，则说明安装成功 1234$ java -version java version "1.7.0_75"Java(TM) SE Runtime Environment (build 1.7.0_75-b13)Java HotSpot(TM) 64-Bit Server VM (build 24.75-b04, mixed mode) Scala的安装Scala 的安装也是一样的，同样解压在app文件夹下，配置环境变量： 12export SCALA_HOME=$WORK_SPACE/scala-2.10.4export PATH=$PATH:$SCALA_HOME/bin 生效环境变量 1$ source .bashrc 查看Scala版本，如果打印出如下版本信息，则说明安装成功 12$ scala -version Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL 至此，我们的Java和Scala的安装工作就结束了，我们需要将安装的目录分发到slave1和slave2中，同时slave1和slave2的环境变量也需要像master中一样配置.分发 1234scp -r ~/app/jdk1.7.0_75 spark@slave1:~/app/scp -r ~/app/jdk1.7.0_75 spark@slave2:~/app/scp -r ~/app/scala-2.10.4 spark@slave1:~/app/scp -r ~/app/scala-2.10.4 spark@slave1:~/app/ 修改slave1和slave2环境的环境变量，可以直接复制。然后在slave1和slave2中分别测试一下有没有安装成功。 安装配置 Hadoop YARN下载解压从官网下载hadoop,这里我们以 hadoop2.6.0 版本为例。 同样我们在~/app中解压 1tar -zxvf hadoop-2.6.0.tar.gz 配置Hadoop同样我们可以在 .bashrc中配置Hadoop的环境变量 123export HADOOP_HOME=/home/spark/app/hadoop-2.6.0export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 需要配置的配置文件都在hadoop根目录下的etc/hadoop中，一共需要配置7个文件： 1234567hadoop-env.shyarn-env.shslavescore-site.xmlhdfs-site.xmlmaprd-site.xmlyarn-site.xml 在hadoop-env.sh中配置JAVA_HOME 1export JAVA_HOME=/home/spark/app/jdk1.7.0_75 在yarn-env.sh中配置JAVA_HOME 1export JAVA_HOME=/home/spark/app/jdk1.7.0_75 在slaves中配置slave节点的ip或者host， 12slave1slave2 修改core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/spark/workspace/hadoop-2.6.0/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/spark/app/hadoop-2.6.0/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/spark/app/hadoop-2.6.0/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;amdnode0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8035&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这是我自己的配置，大家可以根据自己的需要修改，将配置好的hadoop-2.6.0文件夹分发给所有slaves吧。 12scp -r ~/app/hadoop-2.6.0 spark@slave1:~/app/scp -r ~/app/hadoop-2.6.0 spark@slave2:~/app/ 启动Hadoop在 master 上执行以下操作，就可以启动 hadoop 了,因为我们配置了Hadoop的环境变量所以就可以在任意目录下启动Hadoop了。 12start-dfs.sh #启动dfs,如果没有配置的话就是sbin/start-dfs.sh start-yarn.sh #启动yarn,如果没有配置的话就是sbin/start-yarn.sh 验证 Hadoop 是否安装成功可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程： 12345$ jps #run on master3407 SecondaryNameNode3218 NameNode3552 ResourceManager3910 Jps 在每个slave上应该有以下几个进程： 1234$ jps #run on slaves2072 NodeManager2213 Jps1962 DataNode 或者在浏览器中输入 http://master:8088 ，应该有 hadoop 的管理界面出来了，并能看到 slave1 和 slave2 节点。也可以通过 1hadoop dfsadmin -report 查看节点使用情况。 Spark安装下载解压进入官方下载地址下载最新版Spark。我下载的是 spark-1.3.0-bin-hadoop2.4.tgz。 在~/app目录下解压 12tar -zxvf spark-1.3.0-bin-hadoop2.4.tgzmv spark-1.3.0-bin-hadoop2.4 spark-1.3.0 #如果觉得原来的文件名太长了，可以修改下 配置 Sparkspark的配置文件在spark根目录下的conf中, Spark需要修改的配置文件只有两个： 12spark-env.sh slaves 在conf目录下将spark-env.sh.template复制成spark-env.sh 123cd ~/app/spark-1.3.0/conf #进入spark配置目录cp spark-env.sh.template spark-env.sh #从配置模板复制vi spark-env.sh #添加配置内容 在spark-env.sh末尾添加以下内容（这是我的配置，你可以自行修改）： 1234567export SCALA_HOME=/home/spark/app/scala-2.10.4export JAVA_HOME=/home/spark/app/jdk1.7.0_75export HADOOP_HOME=/home/spark/app/hadoop-2.6.0export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopSPARK_MASTER_IP=masterSPARK_LOCAL_DIRS=/home/spark/app/spark-1.3.0SPARK_DRIVER_MEMORY=1G 注：在设置Worker进程的CPU个数和内存大小，要注意机器的实际硬件条件，如果配置的超过当前Worker节点的硬件条件，Worker进程会启动失败。 修改slaves文件下slaves的主机名： 12slave1slave2 将配置好的spark-1.3.0文件夹分发给所有slaves吧 12scp -r ~/app/spark-1.3.0 spark@slave1:~/workspace/scp -r ~/app/spark-1.3.0 spark@slave2:~/workspace/ 启动Spark在spark的根目录下： 1sbin/start-all.sh 验证 Spark 是否安装成功用jps检查，在 master 上应该有以下几个进程： 123456$ jps7949 Jps7328 SecondaryNameNode7805 Master7137 NameNode7475 ResourceManager 在 slave 上应该有以下几个进程： 12345$jps3132 DataNode3759 Worker3858 Jps3231 NodeManager 也可以进入Spark的Web管理页面： http://master:8080 注意：三个节点的防火墙要关掉，不然很容易出错，这里中间很多的细节都没有涉及到，只是个大概的流程，我相信，每个人刚学的时候都不会一次性的成功，但是者未必不是好事，有些坑是需要踩过才知道，这里有很多的坑，祝大家早日脱坑。]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>YARN</tag>
        <tag>分布式环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密登录]]></title>
    <url>%2F2017%2F05%2F31%2FSSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[纸上得来终觉浅，要知此事须躬行在配置集群的时候，ssh免密登录时第一步。其实linux免密登录很简单，四步就可以解决问题 准备假设有两台机器他们的IP和主机名是：M1-IP：10.1.130.2 主机名：m1M2-IP：10.1.130.3 主机名：m2如果想要更改主机名，可以在每台机器的/etc/hostname中更改，但是需要重启生效。 映射主机名每个机器都进入 /etc/hosts ，并添加所有的主机名和IP映射1210.1.130.2 m110.1.130.3 m2 生成公钥执行以下命令，生成公钥，一直回车就行，如果之前有的就输入y就覆盖就行。默认目录是放在 ~/.ssh 下面，名为id_rsa.pub。1ssh-keygen -t rsa 汇总公钥 汇总公钥至同一机器（为了方面下一步），假如在m2中将公钥复制到m1。 1scp id_rsa.pub m1@10.1.1130.2:~/.ssh/id_rsa.pub.m2 将公钥合并至 authorized_keys 1cat id_rsa.pub* &gt;&gt;authorized_keys 分发公钥1scp authorized_keys m2@10.1.1130.3:~/.ssh 大工告成]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>免密登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS常用命令]]></title>
    <url>%2F2017%2F05%2F31%2FHDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[衣带渐宽终不悔，为伊消得人憔悴 文件操作 建立目录12hadoop dfs -mkdir -p /user/hadoop/examples #加上-p是所有目录都要建立eadoop dfs -mkdir /user/hadoop/examples1 #建立examples1目录 删除目录12hadoop dfs -rm -r /user/hadoop/examples #加上-r，删除exampleshadoop dfs -rm -r /user #删除user目录 列出HDFS下的文件(夹) 12hadoop dfs -ls / #查看hdfs根目录下的文件夹(文件)hadoop dfs -ls /user/data #查看某个目录下的文件夹(文件) 查看文件内容 1hadoop dfs -cat /user/data/in/word.txt #查看文件内容，必须是一个文件，不能时目录 将hdfs上的文件(夹)复制到本地的文件系统 123456hadoop dfs -get /user/data rename #将data目录复制到当前执行该命令的本地文件系统，并重命名为renamehadoop dfs -get /user/data /home/user/data rename #将data目录复制到本地指定目录下，并重命名为renamehadoop dfs -get /user/data /home/user/data/core-site.xml /home/user/data/rename.xml #将core-site.xml复制到本地指定目录下，并重命名为rename.xmlhadoop dfs -getmerger /user/data/in merge.xml #将hdfs中某个目录下的的文件合并并下载到本地当前目录 将本地文件系统上传到hdfs上 1hadoop dfs -put file /user/data 管理与更新 执行基本信息, 查看HDFS的基本统计信息: 1hadoop dfsadmin -report HDFS的数据在各个DataNode中的分布可能很不均匀，尤其是在DataNode节点出现故障或新增DataNode节点时。新增数据块时NameNode对DataNode节点的选择策略也有可能导致数据块分布不均匀。用户可以使用命令重新平衡DataNode上的数据块的分布: 1hadoop$bin/start-balancer.sh]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令]]></title>
    <url>%2F2017%2F05%2F31%2FGit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 获取仓库 初始化一个版本仓库1git init Clone远程版本库 1git clone git@xbc.me:wordpress.git 添加远程版本库origin，语法为 git remote add [shortname] [url] 1git remote add origin git@xbc.me:wordpress.git 查看远程仓库 1git remote -v 提交修改 添加当前修改的文件到暂存区 1git add . 如果你自动追踪文件，包括你已经手动删除的，状态为Deleted的文件 1git add -u 提交你的修改 1git commit –m "你的注释" 推送你的更新到远程服务器,语法为 git push [远程名] [本地分支]:[远程分支] 1git push origin master 查看文件状态 1git status 跟踪新文件 1git add readme.txt 从当前跟踪列表移除文件，并完全删除 1git rm readme.txt 仅在暂存区删除，保留文件在当前目录，不再跟踪 1git rm –cached readme.txt 重命名文件 1git mv reademe.txt readme 查看提交的历史记录 1git log 修改最后一次提交注释的，利用–amend参数 1git commit --amend 忘记提交某些修改，下面的三条命令只会得到一个提交 123git commit –m "add readme.txt"git add readme_forgottengit commit –amend 假设你已经使用git add .，将修改过的文件a、b加到暂存区现在你只想提交a文件，不想提交b文件，应该这样 1git reset HEAD b 取消对文件的修改 1git checkout –- readme.txt 分支管理 创建一个分支 1git branch iss53 切换工作目录到iss53 1git chekcout iss53 将上面的命令合在一起，创建iss53分支并切换到iss53 1git chekcout –b iss53 合并iss53分支，当前工作目录为master 1git merge iss53 合并完成后，没有出现冲突，删除iss53分支 1git branch –d iss53 拉去远程仓库的数据，语法为 git fetch [remote-name] 1git fetch fetch 会拉去最新的远程仓库数据，但不会自动到当前目录下，要自动合并 1git pull 查看远程仓库的信息 1git remote show origin 建立本地的dev分支追踪远程仓库的develop分支 1git checkout –b dev origin/develop+ #分支管理]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
</search>